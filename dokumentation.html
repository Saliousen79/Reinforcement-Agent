<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dokumentation - Multi-Agent RL</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.1/font/bootstrap-icons.css">

    <style>
        :root {
            --bg-primary: #0a0a0f;
            --bg-secondary: #12121a;
            --bg-card: #16161f;
            --bg-elevated: #1c1c28;
            --bg-code: #1a1a24;
            --border: #2a2a3a;
            --border-light: #3a3a4a;
            --text-primary: #f0f0f5;
            --text-secondary: #a0a0b0;
            --text-muted: #606070;
            --accent: #6c63ff;
            --accent-hover: #7d75ff;
            --blue: #4dabf7;
            --red: #ff6b6b;
            --yellow: #ffd43b;
            --green: #69db7c;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }
        html { 
            scroll-behavior: smooth;
            background: var(--bg-primary);
        }

        body {
            font-family: 'Inter', -apple-system, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.7;
            -webkit-font-smoothing: antialiased;
            /* LAYOUT FREEZE: Fixiert bei 1920px */
            max-width: 1920px;
            margin: 0 auto;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 0 45px;
        }

        /* Navigation */
        nav {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background: rgba(10, 10, 15, 0.95);
            backdrop-filter: blur(12px);
            border-bottom: 1px solid var(--border);
            padding: 18px 0;
            z-index: 1000;
        }

        nav .nav-container {
            max-width: 1100px;
            margin: 0 auto;
            display: flex;
            justify-content: center;
            gap: 10px;
            padding: 0 32px;
        }

        nav a {
            text-decoration: none;
            color: var(--text-secondary);
            padding: 10px 22px;
            border-radius: 8px;
            font-size: 0.95rem;
            font-weight: 500;
            transition: all 0.2s;
            border: 1px solid transparent;
        }

        nav a:hover {
            background: var(--bg-elevated);
            color: var(--text-primary);
            border-color: var(--border);
        }

        nav a.active {
            background: rgba(108, 99, 255, 0.15);
            color: var(--accent);
            border-color: var(--accent);
        }

        /* Header */
        .page-header {
            padding: 125px 0 52px;
            text-align: center;
            border-bottom: 1px solid var(--border);
        }

        .page-header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 16px;
        }

        .page-header p {
            color: var(--text-secondary);
            font-size: 1.1rem;
            max-width: 750px;
            margin: 0 auto;
        }

        /* Content */
        .content {
            padding: 65px 0;
        }

        .doc-section {
            margin-bottom: 65px;
        }

        .doc-section h2 {
            font-size: 1.6rem;
            font-weight: 600;
            margin-bottom: 22px;
            padding-bottom: 12px;
            border-bottom: 1px solid var(--border);
            color: var(--text-primary);
        }

        .doc-section h3 {
            font-size: 1.2rem;
            font-weight: 600;
            margin: 28px 0 14px;
            color: var(--accent);
        }

        .doc-section p {
            color: var(--text-secondary);
            margin-bottom: 16px;
            line-height: 1.8;
            font-size: 1.02rem;
        }

        .doc-section ul, .doc-section ol {
            color: var(--text-secondary);
            padding-left: 28px;
            margin-bottom: 16px;
        }

        .doc-section li {
            margin-bottom: 10px;
            line-height: 1.7;
            font-size: 1.02rem;
        }
        
        .doc-section strong {
            color: var(--text-primary);
        }

        /* Video Embeds */
        .video-embed {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 15px;
            overflow: hidden;
            margin: 30px 0;
        }

        .video-embed .video-wrapper {
            position: relative;
            padding-bottom: 56.25%;
            background: #000;
        }

        .video-embed iframe,
        .video-embed video {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: 0;
            object-fit: contain;
        }

        .video-embed .video-caption {
            padding: 18px 24px;
            border-top: 1px solid var(--border);
        }

        .video-embed .video-caption h4 {
            font-size: 0.98rem;
            font-weight: 600;
            margin-bottom: 4px;
            color: var(--text-primary);
        }

        .video-embed .video-caption p {
            font-size: 0.88rem;
            color: var(--text-muted);
            margin-bottom: 0;
        }

        /* Code blocks */
        .code-block {
            background: var(--bg-code);
            border: 1px solid var(--border);
            border-radius: 11px;
            padding: 23px 26px;
            margin: 24px 0;
            overflow-x: auto;
            position: relative;
        }
        
        .code-label {
            position: absolute;
            top: 12px;
            right: 18px;
            font-size: 0.75rem;
            color: var(--text-muted);
            text-transform: uppercase;
            font-family: 'JetBrains Mono', monospace;
        }

        .code-block code {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.88rem;
            color: var(--text-primary);
            white-space: pre;
            display: block;
        }
        
        .code-comment { color: #606070; font-style: italic; }
        .code-keyword { color: #c678dd; }
        .code-function { color: #61afef; }
        .code-number { color: #d19a66; }
        .code-string { color: #98c379; }

        .inline-code {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.85rem;
            background: var(--bg-code);
            padding: 2px 8px;
            border-radius: 4px;
            color: var(--accent);
            border: 1px solid var(--border-light);
        }

        /* Tables */
        .table-wrapper {
            overflow-x: auto;
            margin: 26px 0;
            border-radius: 11px;
            border: 1px solid var(--border);
        }

        table {
            width: 100%;
            border-collapse: collapse;
            background: var(--bg-card);
        }

        th, td {
            padding: 17px 24px;
            text-align: left;
            border-bottom: 1px solid var(--border);
        }

        th {
            background: var(--bg-elevated);
            font-weight: 600;
            font-size: 0.88rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-secondary);
        }

        td {
            font-size: 0.98rem;
            color: var(--text-secondary);
        }
        
        td strong { color: var(--text-primary); }

        tr:last-child td {
            border-bottom: none;
        }

        /* Info boxes */
        .info-box {
            background: rgba(22, 22, 31, 0.6);
            border: 1px solid var(--border);
            border-left: 4px solid var(--accent);
            border-radius: 15px;
            padding: 28px 35px;
            margin: 30px 0;
            font-size: 1.05rem;
        }

        .info-box.warning {
            border-left-color: var(--yellow);
            background: rgba(255, 212, 59, 0.05);
        }

        .info-box.success {
            border-left-color: var(--green);
            background: rgba(105, 219, 124, 0.05);
        }

        .info-box h4 {
            font-size: 1.05rem;
            font-weight: 700;
            margin-bottom: 10px;
            color: var(--text-primary);
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .info-box p {
            margin-bottom: 0;
            font-size: 0.98rem;
        }

        /* TOC */
        .toc {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 15px;
            padding: 28px 33px;
            margin-bottom: 52px;
        }

        .toc h3 {
            font-size: 1.02rem;
            margin-bottom: 16px;
            color: var(--text-primary);
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .toc ul {
            list-style: none;
            padding: 0;
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 15px;
        }

        .toc li {
            margin-bottom: 0;
        }

        .toc a {
            color: var(--text-secondary);
            text-decoration: none;
            font-size: 0.98rem;
            transition: color 0.2s;
            display: flex;
            align-items: center;
            gap: 8px;
        }
        
        .toc a::before {
            content: "→";
            color: var(--accent);
            opacity: 0.5;
        }

        .toc a:hover {
            color: var(--accent);
        }

        /* Footer */
        footer {
            padding: 42px 0;
            text-align: center;
            border-top: 1px solid var(--border);
            color: var(--text-muted);
            font-size: 0.88rem;
            margin-top: 52px;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .page-header h1 { font-size: 2rem; }
            nav .nav-container { gap: 4px; padding: 0 16px; }
            nav a { padding: 8px 12px; font-size: 0.8rem; }
            .toc ul { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>

    <nav>
        <div class="nav-container">
            <a href="index.html">Home</a>
            <a href="dokumentation.html" class="active">Dokumentation</a>
            <a href="visualization/index.html">Visualisierung</a>
            <a href="kennzahlen.html">Kennzahlen</a>
        </div>
    </nav>

    <header class="page-header">
        <div class="container">
            <h1>Technische Dokumentation</h1>
            <p>Ein tiefer Einblick in die Architektur, das Trainings-Setup und die mathematischen Grundlagen des Algernon-Projekts.</p>
        </div>
    </header>

    <main class="content">
        <div class="container">

            <div class="toc">
                <h3>Inhaltsverzeichnis</h3>
                <ul>
                    <li><a href="#architektur">1. System-Architektur</a></li>
                    <li><a href="#training">2. Infrastruktur & Training</a></li>
                    <li><a href="#hyperparameter">3. Hyperparameter</a></li>
                    <li><a href="#rewards">4. Reward Shaping</a></li>
                    <li><a href="#observation">5. Observation Space</a></li>
                    <li><a href="#action">6. Action Space</a></li>
                </ul>
            </div>

            <section class="doc-section" id="architektur">
                <h2>1. System-Architektur</h2>
                <p>
                    Das Projekt implementiert eine strikte Trennung von Environment-Logik und Agenten-Training über das 
                    <strong>Gymnasium Interface</strong>. Als Algorithmus kommt <strong>PPO (Proximal Policy Optimization)</strong> 
                    zum Einsatz, da er als On-Policy-Verfahren einen exzellenten Kompromiss 
                    zwischen Trainingsstabilität und Sample Efficiency bietet.
                </p>

                <h3>Netzwerkstruktur (MLP)</h3>
                <p>
                    Da der Observation Space bereits abstrahierte Vektoren statt roher Pixel liefert, ist der Einsatz von Convolutional Neural Networks (CNNs) nicht notwendig.
                    Stattdessen nutze ich ein tiefes <strong>Multi-Layer Perceptron (MLP)</strong>.
                </p>
                
                <ul>
                    <li><strong>Policy Network (Actor):</strong> Zwei Hidden Layers mit je 512 Neuronen.</li>
                    <li><strong>Value Network (Critic):</strong> Spiegelt die Policy-Struktur, teilt aber keine Gewichte (separate Networks).</li>
                    <li><strong>Aktivierungsfunktion:</strong> Tanh (Hyperbolischer Tangens), um stetige Gradienten zu gewährleisten.</li>
                </ul>

                <div class="code-block">
                    <span class="code-label">train.py</span>
                    <code><span class="code-comment"># Konfiguration des PPO-Modells mit Stable Baselines 3</span>
<span class="code-keyword">model</span> = PPO(
    policy=<span class="code-string">"MlpPolicy"</span>,
    env=vec_env,
    learning_rate=<span class="code-number">3e-4</span>,
    n_steps=<span class="code-number">2048</span>,
    batch_size=<span class="code-number">512</span>,
    policy_kwargs=<span class="code-keyword">dict</span>(
        net_arch=[<span class="code-number">512</span>, <span class="code-number">512</span>]  <span class="code-comment"># Deep Network für komplexe Strategien</span>
    )
)</code>
                </div>

                <div class="info-box success">
                    <h4><i class="bi bi-lightbulb"></i> Warum kein CNN?</h4>
                    <p>Durch den Verzicht auf Pixel-Verarbeitung (CNN) und die Nutzung kompakter Feature-Vektoren konnte die Trainingsgeschwindigkeit um den Faktor 10-20 erhöht werden, ohne strategische Tiefe zu verlieren.</p>
                </div>
            </section>

            <section class="doc-section" id="training">
                <h2>2. Infrastruktur & Training</h2>
                <p>
                    Ein zentrales Problem bei Multi-Agent Reinforcement Learning (MARL) ist die hohe Varianz der Trainingsdaten. 
                    Um dies zu kompensieren, setzt das Projekt auf massive Parallelisierung.
                </p>

                <h3>Vektorisierte Umgebungen</h3>
                <p>
                    Statt einer einzelnen Simulations-Instanz nutze ich <strong>16 vektorisierte Umgebungen (SubprocVecEnv)</strong>,
                    die auf separaten CPU-Kernen laufen. Dies ermöglichte das Sammeln von über <strong>100 Millionen Experience-Steps</strong>
                    in einer akzeptablen Zeit.
                </p>

                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>Parameter</th>
                                <th>Wert</th>
                                <th>Bedeutung</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Total Timesteps</strong></td>
                                <td><span class="inline-code">100.000.000</span></td>
                                <td>Massive Datenmenge für statistische Signifikanz.</td>
                            </tr>
                            <tr>
                                <td><strong>Parallel Envs</strong></td>
                                <td><span class="inline-code">16</span></td>
                                <td>Anzahl gleichzeitiger Simulationen (Multiprocessing).</td>
                            </tr>
                            <tr>
                                <td><strong>Hardware</strong></td>
                                <td><span class="inline-code">CPU-Cluster</span></td>
                                <td>Optimiert für parallele Logik-Berechnungen statt GPU-Rendering.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <section class="doc-section" id="hyperparameter">
                <h2>3. Hyperparameter</h2>
                <p>
                    Die Hyperparameter wurden speziell angepasst, um das "Exploration vs. Exploitation"-Dilemma in einer kompetitiven Umgebung zu lösen.
                    Besonders der <strong>Entropy Coefficient</strong> wurde erhöht, um ein vorzeitiges Konvergieren auf suboptimale Strategien zu verhindern.
                </p>

                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>Hyperparameter</th>
                                <th>Wert</th>
                                <th>Begründung</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Learning Rate</td>
                                <td><span class="inline-code">3e-4</span></td>
                                <td>Standard für Adam Optimizer, bietet stabilen Gradient Descent.</td>
                            </tr>
                            <tr>
                                <td>Batch Size</td>
                                <td><span class="inline-code">512</span></td>
                                <td>Erhöht für stabilere Updates in hoch-dynamischen Umgebungen.</td>
                            </tr>
                            <tr>
                                <td>N_Steps</td>
                                <td><span class="inline-code">2048</span></td>
                                <td>Langer "Lookahead"-Horizont für strategische Planung.</td>
                            </tr>
                            <tr>
                                <td>Entropy Coef</td>
                                <td><span class="inline-code">0.03</span></td>
                                <td><strong>Wichtig:</strong> Zwingt den Agenten zum Experimentieren (Exploration).</td>
                            </tr>
                            <tr>
                                <td>Gamma (Discount)</td>
                                <td><span class="inline-code">0.99</span></td>
                                <td>Fokus auf langfristige Belohnungen (Sieg) statt sofortiger Punkte.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <section class="doc-section" id="rewards">
                <h2>4. Reward Shaping (Forschungskern)</h2>
                <p>
                    In Reinforcement Learning ist die Definition der Belohnungsfunktion (Reward Function) oft anspruchsvoller als der Lernalgorithmus selbst.
                    Das <strong>Temporal Credit Assignment Problem</strong> – also die Frage, welche vergangene Aktion für den aktuellen Erfolg verantwortlich ist – wurde durch drei verschiedene Profile untersucht.
                </p>

                <div class="info-box">
                    <h4><i class="bi bi-layers-fill"></i> Die 3 Profile</h4>
                    <ul>
                        <li><strong>Charlie (Sparse):</strong> Erhält Rewards <em>nur</em> für Capture (+100) und Sieg/Niederlage (±50). Extrem schwer zu lernen, da Feedback sehr selten ist.</li>
                        <li><strong>Gordon (Dense/Micromanager):</strong> Erhält kontinuierliches Feedback für fast alles (Bewegung zur Flagge +0.2, Flagge berühren +10.0, etc.). Führt oft zu "Reward Hacking" (z.B. im Kreis laufen für Punkte).</li>
                        <li><strong>Algernon (Balanced):</strong> Belohnt nur <em>signifikante</em> Meilensteine (Tackle auf Träger +8.0, Flagge zurückbringen +5.0), fördert strategisches Denken.</li>
                    </ul>
                </div>

                <div class="info-box warning">
                    <h4>Erkenntnis: Reward Hacking</h4>
                    <p>
                        Agenten mit "Dense Rewards" (Gordon) neigten dazu, die Flagge aufzuheben und sofort fallen zu lassen, um den "Pickup Reward" (+10.0) wiederholt zu farmen, statt das Spielziel zu verfolgen.
                    </p>
                </div>
            </section>

            <section class="doc-section" id="observation">
                <h2>5. Observation Space (Feature Engineering)</h2>
                <p>
                    Ein Netz lernt "Gegner ist bei x=20" schlecht, aber "Gegner ist 5 Meter vor mir" sehr gut.
                    Daher erhält der Agent statt absoluter Koordinaten ein <strong>relatives, ego-zentrisches Weltbild</strong>.
                    Der Observation Vector besteht aus <strong>31 kontinuierlichen Werten</strong>.
                </p>

                <h3>Inhalt des Vektors:</h3>
                <ul>
                    <li><strong>Relative Vektoren (Normalized):</strong> Zur eigenen Basis, zur eigenen Flagge, zur gegnerischen Flagge.</li>
                    <li><strong>Permutation Invariance:</strong> Die Inputs für die zwei Gegner sind nach Distanz sortiert (Nächster Gegner zuerst). So lernt der Agent "Umgang mit Bedrohung", egal welche ID der Gegner hat.</li>
                    <li><strong>Status Flags:</strong> Bin ich gestunned? Habe ich die Flagge? Ist meine Flagge sicher?</li>
                    <li><strong>LIDAR-Sensorik:</strong> Distanz zu den vier Spielfeldgrenzen (Wanderkennung).</li>
                </ul>
            </section>

            <section class="doc-section" id="action">
                <h2>6. Action Space</h2>
                <p>
                    Der Action Space ist <strong>diskret (Discrete 6)</strong>. Die Reduktion auf diskrete Aktionen vereinfacht den Entscheidungsbaum für das neuronale Netz massiv im Vergleich zu kontinuierlichen Actions (Box).
                </p>

                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>ID</th>
                                <th>Aktion</th>
                                <th>Effekt</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><span class="inline-code">0-3</span></td>
                                <td>Bewegung</td>
                                <td>Oben, Unten, Links, Rechts (mit Kollisionsabfrage)</td>
                            </tr>
                            <tr>
                                <td><span class="inline-code">4</span></td>
                                <td>Idle</td>
                                <td>Warten / Position halten</td>
                            </tr>
                            <tr>
                                <td><span class="inline-code">5</span></td>
                                <td><strong>Tackle</strong></td>
                                <td>Spezial-Aktion: Betäubt Gegner in Reichweite (Stun) und zwingt zum Flaggen-Abwurf. Hat einen Cooldown von 3.25s.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="info-box success">
                    <h4>Strategische Tiefe</h4>
                    <p>Die "Tackle"-Aktion führt zu komplexem Verhalten: Agenten lernen, Gegner nicht nur zu blockieren, sondern gezielt dann anzugreifen, wenn diese die Flagge tragen.</p>
                </div>
            </section>

        </div>
    </main>

    <footer>
        <div class="container">
            <p>Project Algernon — Multi-Agent Reinforcement Learning Study</p>
        </div>
    </footer>

</body>
</html>