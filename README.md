# ğŸš© Capture the Flag - Multi-Agent Reinforcement Learning

Ein vollstÃ¤ndiges 2v2 Capture the Flag Multi-Agent RL Projekt mit PPO-Training, 3D-Visualisierung und interaktivem Analytics Dashboard.

![CTF Banner](https://img.shields.io/badge/Multi--Agent-RL-blue) ![Python](https://img.shields.io/badge/Python-3.8+-green) ![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ¯ Ãœberblick

Dieses Projekt implementiert ein vollstÃ¤ndiges Reinforcement Learning System fÃ¼r Capture the Flag. Zwei Teams mit je 2 Agenten lernen kooperativ zu spielen, Strategien zu entwickeln und komplexe Spielsituationen zu meistern.

**Highlights:**
- ğŸ¤– **3 trainierte Modelle** (50M, 200M, 250M Zeitschritte)
- ğŸ® **3D Replay Viewer** mit Three.js
- ğŸ“Š **Live Dashboard** fÃ¼r Training-Metriken
- ğŸ“– **Interaktive Dokumentation** mit SpielerklÃ¤rungen
- ğŸ”§ **VollstÃ¤ndig reproduzierbar**

## ğŸŒŸ Features

- **Custom PettingZoo Environment** (24x24 Grid mit Walls & Safe Zones)
- **PPO Training** mit Stable-Baselines3
- **Team Cooperation**: Agenten lernen koordiniert zu spielen
- **Tackle Mechanik**: BetÃ¤ube Gegner strategisch
- **Realistic Gameplay**: Speed-Penalty fÃ¼r FlaggentrÃ¤ger
- **JSON Replay System**: Speichere und analysiere jedes Spiel

## ğŸ“ Projektstruktur

```
.
â”œâ”€â”€ index.html             # ğŸ  Landing Page mit Navigation
â”œâ”€â”€ docs/                  # ğŸ“– Interaktive Dokumentation
â”‚   â””â”€â”€ index.html         #    - SpielerklÃ¤rung
â”‚                          #    - Modell-PrÃ¤sentationen
â”‚                          #    - Video-Embeds
â”œâ”€â”€ training/              # ğŸ“ Training Code & Modelle
â”‚   â”œâ”€â”€ environment.py     #    - CTF Environment (PettingZoo)
â”‚   â”œâ”€â”€ train.py           #    - PPO Training Script
â”‚   â”œâ”€â”€ export_replay.py   #    - Replay Export Tool
â”‚   â”œâ”€â”€ models/            #    ğŸ“¦ Finale Modelle:
â”‚   â”‚   â”œâ”€â”€ Night_1.zip           - ~50M Steps (Baseline)
â”‚   â”‚   â”œâ”€â”€ Night_200M.zip        - 200M Steps (Advanced)
â”‚   â”‚   â””â”€â”€ Algernon_250M.zip     - 250M Steps (Best)
â”‚   â””â”€â”€ logs/              #    ğŸ“Š TensorBoard Logs
â”œâ”€â”€ visualization/         # ğŸ® 3D Replay Viewer
â”‚   â”œâ”€â”€ index.html         #    - Three.js Visualisierung
â”‚   â”œâ”€â”€ main.js            #    - Rendering Engine
â”‚   â””â”€â”€ replays/           #    - JSON Replays
â””â”€â”€ dashboard/             # ğŸ“Š Analytics Dashboard
    â”œâ”€â”€ index.html         #    - Live Training Metriken
    â””â”€â”€ dashboard.js       #    - Chart.js Integration
```

## ğŸš€ Quick Start

### Option A: Nur die Visualisierung nutzen (empfohlen)

Die einfachste Methode, um die trainierten Modelle zu sehen:

```bash
# 1. Repository klonen
git clone https://github.com/yourusername/Reinforcement-Agent.git
cd Reinforcement-Agent

# 2. Lokalen Server starten
python -m http.server 8000

# 3. Browser Ã¶ffnen
# ğŸ  Landing Page:     http://localhost:8000
# ğŸ“– Dokumentation:    http://localhost:8000/docs/
# ğŸ® 3D Viewer:        http://localhost:8000/visualization/
# ğŸ“Š Dashboard:        http://localhost:8000/dashboard/
```

Die 3 finalen Modelle (Night_1, Night_200M, Algernon_250M) und ein Replay sind bereits im Repository enthalten!

### Option B: Eigenes Training (Fortgeschritten)

FÃ¼r eigene Experimente und neues Training:

```bash
# 1. Virtual Environment erstellen
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# 2. Dependencies installieren
cd training
pip install -r requirements.txt

# 3. Training starten (z.B. 1 Million Steps)
python train.py --timesteps 1000000 --envs 4 --name MyModel

# 4. Replay aus trainiertem Modell erstellen
python export_replay.py --model models/MyModel_final.zip

# 5. Server starten und Replay ansehen
cd ..
python -m http.server 8000
# Ã–ffne http://localhost:8000/visualization/
```

### ğŸ¬ Replays erstellen

```bash
cd training

# Demo Episode (ohne Modell - zufÃ¤llige Aktionen)
python export_replay.py --demo

# Mit einem der trainierten Modelle
python export_replay.py --model models/Algernon_250M.zip --seed 42
python export_replay.py --model models/Night_200M.zip --seed 123
python export_replay.py --model models/Night_1.zip

# Replays werden in visualization/replays/ gespeichert
```

## ğŸ¤– Trainierte Modelle

Das Repository enthÃ¤lt 3 finale Modelle mit unterschiedlichen Trainingsgraden:

| Modell | Zeitschritte | Performance | Verwendung |
|--------|-------------|-------------|------------|
| **Night_1** | ~50M | Baseline | Gute Grundstrategien, defensiv |
| **Night_200M** | 200M | Advanced | Ausgewogen, adaptive Taktiken |
| **Algernon_250M** | 250M | Elite | Beste Performance, kreative Strategien |

**Alle Modelle sind spielbereit!** Einfach Server starten und im 3D Viewer ansehen.

## ğŸ”„ Reproduzierbarkeit

Das Projekt ist vollstÃ¤ndig reproduzierbar. Um das Training zu wiederholen:

1. **Environment Setup:** Alle Parameter in `training/environment.py`
2. **Training Config:** Hyperparameter in `training/train.py`
3. **Reproduktion:** Nutze den gleichen Seed fÃ¼r deterministische Ergebnisse

```bash
# Exakte Reproduktion eines Trainings
python train.py --timesteps 1000000 --envs 4 --name Experiment1 --seed 42
```

**Wichtige Dateien:**
- `training/requirements.txt` - Exakte Package-Versionen
- `training/environment.py` - Environment-Konfiguration
- `training/train.py` - Training-Loop und PPO-Config

## ğŸ® Environment Details

### Observation Space (22 Werte)
- Eigene Info (x, y, has_flag, is_stunned, cooldown)
- Teammate (x, y, has_flag)
- Gegner (x, y, has_flag) x2
- Flaggen (x, y, at_base) x2
- Scores (own, enemy)

### Action Space
- 0-3: Hoch, Runter, Links, Rechts
- 4: Nichts tun
- 5: Tackle

### Spielmechaniken
- Flagge aufnehmen in Gegner-Base
- FlaggentrÃ¤ger 20% langsamer
- Tackle stunned Gegner fÃ¼r 1.5s (Cooldown: 5s)
- Safe Zone in eigener Base
- Sieg bei 3 Captures oder hÃ¶chster Score nach 500 Steps

### Rewards
| Aktion | Reward |
|--------|--------|
| Flagge aufnehmen | +5 |
| Eigene Flagge zurÃ¼cksetzen | +8 |
| Gegner stunnen | +3 |
| FlaggentrÃ¤ger stunnen | +13 |
| Capture | +50 |
| Spiel gewonnen | +100 |
| Spiel verloren | -50 |
| Pro Schritt | -0.01 |

## Konfiguration

### Environment Parameter

In `train.py` â†’ `make_env()`:

```python
CaptureTheFlagEnv(
    grid_size=24,
    max_steps=500,
    win_score=3,
    stun_duration=30,
    tackle_cooldown=100,
    tackle_range=1.5,
    carrier_speed_penalty=0.2
)
```

### PPO Hyperparameter

In `train.py` â†’ `PPO()`:

```python
PPO(
    policy="MlpPolicy",
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    gamma=0.99,
    ent_coef=0.01
)
```

## ğŸ› ï¸ Troubleshooting

### Training-Probleme

**"ModuleNotFoundError" beim Training:**
```bash
cd training
pip install -r requirements.txt
```

**Training zu langsam:**
- Reduziere `--timesteps` fÃ¼r Tests (z.B. 100000)
- ErhÃ¶he `--envs` fÃ¼r mehr parallele Environments (z.B. 8 oder 16)

**Modell lernt nicht:**
- Check TensorBoard: `tensorboard --logdir training/logs`
- Reward sollte im Laufe der Zeit steigen
- Bei Problemen: Hyperparameter in `train.py` anpassen

### Visualisierungs-Probleme

**3D Viewer zeigt nichts:**
1. Erstelle ein Demo-Replay: `python export_replay.py --demo`
2. Server muss im Projekt-Root laufen (nicht in `visualization/`)
3. Check Browser-Console fÃ¼r Fehler

**Dashboard zeigt keine Daten:**
- Mindestens einmal Training starten: `python train.py --timesteps 10000`
- Oder `training/data/training_logs.json` manuell erstellen

**Replay lÃ¤dt nicht:**
- JSON-Datei muss in `visualization/replays/` liegen
- Check Browser DevTools Network Tab fÃ¼r 404-Fehler

### Allgemeine Probleme

**Server startet nicht auf Port 8000:**
```bash
# Nutze einen anderen Port
python -m http.server 8080
# Dann Ã¶ffne http://localhost:8080
```

**Performance-Probleme im 3D Viewer:**
- Nutze einen modernen Browser (Chrome/Firefox empfohlen)
- Reduziere Replay-LÃ¤nge fÃ¼r kÃ¼rzere Ladezeiten

## ğŸ“š Dokumentation

FÃ¼r detaillierte Informationen:
- ğŸ“– **Interaktive Docs:** Ã–ffne `http://localhost:8000/docs/` nach dem Server-Start
- ğŸ® **Spielmechaniken:** Siehe `docs/index.html` fÃ¼r visuelle ErklÃ¤rungen
- ğŸ¤– **Modell-Details:** Jedes Modell hat eine eigene Sektion in der Dokumentation

## ğŸ¤ Contributing

BeitrÃ¤ge sind willkommen! Um beizutragen:

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/AmazingFeature`)
3. Commit deine Ã„nderungen (`git commit -m 'Add some AmazingFeature'`)
4. Push zum Branch (`git push origin feature/AmazingFeature`)
5. Ã–ffne einen Pull Request

## ğŸ“„ Lizenz

MIT License - siehe [LICENSE](LICENSE) Datei

## ğŸ™ Credits & Technologie-Stack

**Frameworks & Libraries:**
- [PettingZoo](https://pettingzoo.farama.org/) - Multi-Agent Environment Framework
- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) - PPO Implementation
- [Three.js](https://threejs.org/) - 3D Visualisierung
- [Chart.js](https://www.chartjs.org/) - Dashboard Charts

**Inspiration:**
- OpenAI Hide and Seek
- DeepMind's Multi-Agent Research
- Classic CTF Game Design

---

**Made with â¤ï¸ using Reinforcement Learning**

FÃ¼r Fragen oder Feedback, Ã¶ffne ein [Issue](https://github.com/yourusername/Reinforcement-Agent/issues)!
