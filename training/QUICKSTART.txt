â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸš€ QUICK START: Portfolio Experiments
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1ï¸âƒ£ ALLE 3 MODELLE TRAINIEREN (Empfohlen)

   cd training
   python train_all_experiments.py

   â±ï¸  Dauer: ~6-12 Stunden
   ğŸ’¾ Checkpoints: Bei 10M, 20M, ..., 100M Steps
   ğŸ“‚ Output: training/models/


2ï¸âƒ£ ODER: NUR 1 MODELL TESTEN (Schneller)

   # Micromanager (Heavy Shaping)
   python train.py --name Test_Micro --profile micromanager --timesteps 10000000

   # Sparse (Minimalist)
   python train.py --name Test_Sparse --profile sparse --timesteps 10000000

   # Balanced (Current Best)
   python train.py --name Test_Balanced --profile balanced --timesteps 10000000

   â±ï¸  Dauer: ~30-60 Minuten (nur 10M Steps)


3ï¸âƒ£ REPLAYS ERSTELLEN

   python create_checkpoint_replays.py

   ğŸ“‚ Output: training/replays/
   ğŸ¬ 9 JSON-Dateien (3 Modelle Ã— 3 Checkpoints)


4ï¸âƒ£ ERGEBNISSE ANSEHEN

   # TensorBoard
   tensorboard --logdir training/logs
   â†’ http://localhost:6006

   # Dashboard
   â†’ Lade Replay-JSON in dein Dashboard


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ“Š DIE 3 REWARD-PROFILE IM VERGLEICH
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MICROMANAGER:  Belohnt ALLES (Pickups, Distance, Tackles, ...)
               â†’ Schnelles Lernen, aber Gefahr von Reward Hacking

SPARSE:        Belohnt NUR Captures & Win/Loss
               â†’ Langsames Lernen, kreative Strategien

BALANCED:      Belohnt kritische Momente (Carrier Distance, Defense)
               â†’ Best Practice - Kompromiss zwischen Speed & Strategie


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ”§ TROUBLESHOOTING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âŒ "ModuleNotFoundError"
   â†’ pip install -r requirements.txt

âŒ Training zu langsam
   â†’ Reduziere --timesteps auf 5000000 (5M)
   â†’ Oder erhÃ¶he --envs auf 16

âŒ Checkpoints fehlen
   â†’ Training wurde abgebrochen
   â†’ Oder cleanup_checkpoints=True (sollte False sein!)

âŒ Sparse lernt NICHTS
   â†’ Das ist normal! Sparse braucht VIEL lÃ¤nger (20M+ Steps)
   â†’ Sei geduldig oder wechsle zu Balanced


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ“ FÃœR DEIN PORTFOLIO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Zeige die 3 verschiedenen Reward-Strukturen (Code-Snippet)
âœ… Vergleiche TensorBoard-Kurven (Screenshot)
âœ… Analysiere Replays (Video oder GIFs)
âœ… Diskutiere Ergebnisse: Welches Profil war am besten?
âœ… ErwÃ¤hne Trade-offs (Speed vs. Strategie)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ¯ VIEL ERFOLG!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Mehr Details: Siehe EXPERIMENTS_README.md
