# Portfolio Experiments: 3 Reward Structures

Dieses Experiment vergleicht **3 verschiedene Reward-Strategien** fÃ¼r das Capture the Flag Reinforcement Learning Spiel.

## ğŸ¯ Experiment-Ziel

**Forschungsfrage:** Wie beeinflusst die Reward-Struktur das Lernverhalten und die finale Performance?

## ğŸ“Š Die 3 Experimente

### 1ï¸âƒ£ **Micromanager** (Heavy Shaping)
Belohnt **jeden kleinen Schritt** in die richtige Richtung.

**Rewards:**
- âœ… Capture: +50.0
- âœ… Win/Loss: Â±20.0
- âœ… **Flag Pickup:** +10.0
- âœ… **Distance to Flag:** +0.2 (ALLE Agenten)
- âœ… **Carrier Distance:** +0.3 (Richtung Base)
- âœ… **Tackle Any:** +3.0 (Jeder Tackle)
- âœ… **Tackle Flag Carrier:** +8.0
- âœ… **Flag Return:** +5.0
- âœ… **Distance to Carrier:** +0.15 (Defense)
- âš ï¸ **Step Penalty:** -0.01 (Anti-Idle)

**Hypothese:** Lernt schneller, kÃ¶nnte aber in lokalen Optima hÃ¤ngenbleiben (reward hacking).

---

### 2ï¸âƒ£ **Sparse** (Pure Minimalist)
Belohnt **NUR das Endergebnis** - keine Hilfe wÃ¤hrend des Spiels.

**Rewards:**
- âœ… Capture: +100.0
- âœ… Win/Loss: Â±50.0
- âŒ Alles andere: **0.0**

**Hypothese:** Braucht lÃ¤nger zum Lernen, entwickelt aber kreative Strategien.

---

### 3ï¸âƒ£ **Balanced** (Current Best Practice)
Belohnt **kritische Momente**, aber nicht jeden Schritt.

**Rewards:**
- âœ… Capture: +100.0
- âœ… Win/Loss: Â±30.0
- âœ… **Carrier Distance:** +0.1 (NUR wenn Flagge getragen wird)
- âœ… **Tackle Flag Carrier:** +8.0
- âœ… **Flag Return:** +5.0

**Hypothese:** Bester Kompromiss - schnelles Lernen + strategische Tiefe.

---

## ğŸš€ Training starten

### Option 1: Alle 3 Experimente automatisch (empfohlen)

```bash
cd training
python train_all_experiments.py
```

**Dauer:** ~6-12 Stunden (je nach CPU)
**Checkpoints:** Bei 10M, 20M, 30M, ..., 100M Steps

---

### Option 2: Einzelne Experimente

#### Micromanager
```bash
python train.py --name Micromanager --profile micromanager --timesteps 100000000
```

#### Sparse
```bash
python train.py --name Sparse --profile sparse --timesteps 100000000
```

#### Balanced
```bash
python train.py --name Balanced --profile balanced --timesteps 100000000
```

---

## ğŸ“ Resultierende Struktur

Nach dem Training:

```
training/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ Micromanager_10000000_steps.zip    # 10M Checkpoint
â”‚   â”œâ”€â”€ Micromanager_50000000_steps.zip    # 50M Checkpoint
â”‚   â”œâ”€â”€ Micromanager_final.zip             # 100M Final
â”‚   â”œâ”€â”€ Sparse_10000000_steps.zip
â”‚   â”œâ”€â”€ Sparse_50000000_steps.zip
â”‚   â”œâ”€â”€ Sparse_final.zip
â”‚   â”œâ”€â”€ Balanced_10000000_steps.zip
â”‚   â”œâ”€â”€ Balanced_50000000_steps.zip
â”‚   â””â”€â”€ Balanced_final.zip
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ PPO_Micromanager/                  # TensorBoard Daten
â”‚   â”œâ”€â”€ PPO_Sparse/
â”‚   â””â”€â”€ PPO_Balanced/
â””â”€â”€ replays/
    â”œâ”€â”€ Micromanager_10M.json              # Nach Replay-Generierung
    â”œâ”€â”€ Micromanager_50M.json
    â”œâ”€â”€ Micromanager_100M.json
    â””â”€â”€ ... (9 Replays total)
```

---

## ğŸ¬ Replays erstellen

Nach dem Training, Replays fÃ¼r alle Checkpoints generieren:

```bash
python create_checkpoint_replays.py
```

**Output:** 9 JSON-Replays (3 Modelle Ã— 3 Checkpoints)

---

## ğŸ“ˆ Analyse & Vergleich

### 1. TensorBoard starten

```bash
tensorboard --logdir training/logs
```

**Vergleiche:**
- Mean Reward Ã¼ber Zeit
- Episode Length
- Learning Curve

### 2. Dashboard verwenden

Ã–ffne dein Dashboard und lade die Replays:
- Micromanager_10M.json vs Sparse_10M.json vs Balanced_10M.json
- Vergleiche Spielstil und Taktiken

### 3. Metriken analysieren

In `training_logs.json` findest du:
- Reward-Verlauf
- Episode Length
- Progress-Tracking

---

## ğŸ”¬ Erwartete Ergebnisse

| Metrik | Micromanager | Sparse | Balanced |
|--------|--------------|--------|----------|
| **Lerngeschwindigkeit** | âš¡ Schnell | ğŸ¢ Langsam | ğŸš€ Mittel |
| **Finale Performance** | ? | ? | ? |
| **Spielstil** | Aggressiv | Kreativ | Strategisch |
| **Reward Hacking** | âš ï¸ Risiko | âœ… Sicher | âœ… Sicher |

---

## ğŸ“ Portfolio-Dokumentation

### Was du zeigen kannst:

1. **Problem Statement:** "Wie beeinflusst Reward Shaping das RL-Lernen?"
2. **Experiment-Design:** 3 verschiedene Reward-Strukturen
3. **Implementation:** Code-Snippets von `REWARD_PROFILES`
4. **Ergebnisse:** TensorBoard Grafiken + Replay-Videos
5. **Diskussion:** Welches Profil war am besten? Warum?

### Beispiel-Grafiken:

- Reward-Kurven aller 3 Modelle (Ã¼bereinander)
- Vergleich bei 10M, 50M, 100M
- Heatmap: Wo bewegen sich die Agenten? (aus Replays)

---

## âš™ï¸ Troubleshooting

### Training dauert zu lange?
- Reduziere `--timesteps` auf 10M fÃ¼r schnelle Tests
- ErhÃ¶he `--envs` auf 16 (wenn genug CPU-Kerne)

### Modell lernt nicht?
- Check TensorBoard: Ist der Reward steigend?
- Sparse braucht VIEL lÃ¤nger - sei geduldig!

### Checkpoint fehlt?
- Check `cleanup_checkpoints=False` in `train.py`
- MÃ¶glicherweise wurde Training abgebrochen

---

## ğŸ“ Wissenschaftliche Quellen

FÃ¼r deine Dokumentation:

1. **Ng, A. Y., Harada, D., & Russell, S. (1999).** "Policy invariance under reward transformations: Theory and application to reward shaping."
2. **OpenAI Spinning Up:** https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#reward-shaping
3. **Stable Baselines3 Docs:** https://stable-baselines3.readthedocs.io/

---

## âœ¨ Viel Erfolg!

Bei Fragen oder Problemen, check die Logs oder Ã¶ffne ein Issue im Repo.

**Happy Training! ğŸš€**
